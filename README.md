# ğŸ¯ VLC Gesture Controled  
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)  [![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)](https://pytorch.org/)  [![MediaPipe](https://img.shields.io/badge/MediaPipe-Hands-green)](https://developers.google.com/mediapipe)  [![OpenCV](https://img.shields.io/badge/OpenCV-4.x-yellow)](https://opencv.org/)  

**Gesture-controlled VLC using MediaPipe + Neural Network**  


## ğŸ“º Demo
*(Coming soon)*  


---

## âœ‹ Features
| Gesture        | Action            | Trigger type         |
| -------------- | ----------------- | -------------------- |
| palm           | base/activation   | gating/activation    |
| fist           | play/pause toggle | one-shot per session |
| two_fingers    | mute/unmute       | one-shot per session |
| finger_up      | volume up         | continuous           |
| finger_down    | volume down       | continuous           |

> **Note:** `palm` arms the system for the next gesture.

---

## ğŸ›  How It Works (Architecture)
```mermaid
flowchart LR
    A[Camera] --> B[MediaPipe Hands<br>(landmark extraction)]
    B --> C[Preprocessing]
    C --> D[Neural Net Classifier<br>(PyTorch)]
    D --> E[Gesture FSM<br>(Palm-gated logic)]
    E --> F[VLC Controller<br>(Hotkeys / HTTP API)]
```
- **One-shot**: `fist`, `two_fingers` â€” triggered once per palm session.  
- **Continuous**: `finger_up`, `finger_down` â€” repeat until palm shown again or hand removed.

---

## ğŸ¬ VLC Media Player Setup (Step-by-step)

### 1. Install VLC (Windows)
Download: [VLC Official Site](https://www.videolan.org/vlc/)  

### 2. Enable Control

#### **a) Hotkeys** *(default fallback)*  
- Space â†’ Play/Pause  
- â†‘ / â†“ â†’ Volume Up/Down  
- M â†’ Mute/Unmute  

#### **b) HTTP Interface** *(recommended for precision)*  
1. Open VLC â†’ Tools â†’ Preferences â†’ Show All (bottom-left).  
2. Interface â†’ Main Interfaces â†’ Check **Web**.  
3. Expand Main Interfaces â†’ **Lua** â†’ Set *Lua HTTP* password (`vlcpassword` default).  
4. Default port: `8080`.  
5. Launch VLC with HTTP enabled:  
   ```bash
   vlc --intf http --http-password vlcpassword
   ```  
6. Test: Visit `http://localhost:8080` in browser.

---

## ğŸ’» Project Installation & Setup
```bash
# Clone repo
git clone <your_repo_url>
cd <repo_name>

# (Recommended) Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/macOS

# Install dependencies
pip install -r requirements.txt
```

**Core dependencies:**  
```bash
pip install opencv-python mediapipe torch joblib psutil pyautogui requests
```

---

## ğŸ“¦ Model: Training & Data Collection

### ğŸ“Š Dataset
- **Gestures**: 5 (`palm`, `fist`, `finger_up`, `finger_down`, `two_fingers`)  
- **Samples**: 200 live captures per gesture  
- **Augmentation** (Ã—5 per sample):  
  - Rotation: Â±15Â°  
  - Scaling: 0.9Ã— â€“ 1.1Ã—  
  - Gaussian Noise: mean=0, std=0.01  
- **Total**: 1200 images per gesture â†’ **6000 samples** overall  
- **Split**: 80% training, 20% validation  

### ğŸ– Input Features
- 21 hand landmarks Ã— 3 coordinates (x, y, z) = **63 features**  

### ğŸ§  Model Architecture (GestureNet)
- Input layer: 63 â†’ 63  
- Hidden Layer 1: 63 â†’ 128 (ReLU)  
- Hidden Layer 2: 128 â†’ 128 (ReLU)  
- Output layer: 128 â†’ 5  

### âš™ Training Setup
- **Loss**: CrossEntropyLoss (multi-class classification)  
- **Optimizer**: Adam, lr = 0.001  
- **Epochs**: 50  
- **Batch size**: 32  
- **Device**: GPU (if available)  

### ğŸ“ˆ Performance (averages across epochs)
- Loss: **0.0275**  
- Accuracy: **99.10%**  
- Precision: **99.24%**  
- Recall: **99.11%**  
- F1 Score: **99.10%**  

Outputs:
- `models/gesture_model.pth`  
- `models/gesture_labels.pkl`  

---

## ğŸ‘€ Test/Preview: Live Recognition
```bash
python code/recognize.py
```
Shows:
- Predicted gesture
- FPS
- Confidence score  

Press **ESC** to quit.  

---

## ğŸ¯ VLC Control Runtime
```bash
python main.py
```
- Detects VLC media playback.  
- Prompts user to enable webcam gesture control.  
- Starts `gesture_vlc.py` for real-time control.  

Palm â†’ Action mapping:
- Palm â†’ Fist â†’ Play/Pause  
- Palm â†’ Two fingers â†’ Mute/Unmute  
- Palm â†’ Finger up â†’ Volume up (continuous)  
- Palm â†’ Finger down â†’ Volume down (continuous)  

---

## ğŸ“‚ File-by-File Explanation
- **train_model.py** â€” Data collection, augmentation, training.  
- **recognize.py** â€” Test trained model with webcam.  
- **gesture_vlc.py** â€” Main recognition + palm-gated gesture FSM.  
- **vlc_controller.py** â€” VLC control (HTTP API or hotkeys).  
- **main.py** â€” VLC monitor + gesture process manager.  
- **metrics_logger.py** â€” Logs training, inference, gesture usage.  
- **plot_training_metrics.py** â€” Plots curves (Loss, Accuracy, Precision, Recall, F1).  


## ğŸ™ Acknowledgements
- [MediaPipe Hands](https://developers.google.com/mediapipe)  
- [PyTorch](https://pytorch.org/)  
- [OpenCV](https://opencv.org/)  
- [VLC](https://www.videolan.org/vlc/)  
